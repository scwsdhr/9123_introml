
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{lab07\_music\_partial}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{lab-7-neural-networks-for-music-classification}{%
\section{Lab 7: Neural Networks for Music
Classification}\label{lab-7-neural-networks-for-music-classification}}

In addition to the concepts in the \href{./mnist_neural.ipynb}{MNIST
neural network demo}, in this lab, you will learn to: * Load a file from
a URL * Extract simple features from audio samples for machine learning
tasks such as speech recognition and classification * Build a simple
neural network for music classification using these features * Use a
callback to store the loss and accuracy history in the training process
* Optimize the learning rate of the neural network

To illustrate the basic concepts, we will look at a relatively simple
music classification problem. Given a sample of music, we want to
determine which instrument (e.g.~trumpet, violin, piano) is playing.
This dataset was generously supplied by
\href{http://steinhardt.nyu.edu/faculty/Juan_Pablo_Bello}{Prof.~Juan
Bello} at NYU Stenihardt and his former PhD student Eric Humphrey (now
at Spotify). They have a complete website dedicated to deep learning
methods in music informatics:

http://marl.smusic.nyu.edu/wordpress/projects/feature-learning-deep-architectures/deep-learning-python-tutorial/

You can also check out Juan's course.

    \hypertarget{loading-the-keras-package}{%
\subsection{Loading the Keras package}\label{loading-the-keras-package}}

We begin by loading keras and the other packages

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{keras}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/usr/local/lib/python3.6/site-packages/h5py/\_\_init\_\_.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from .\_conv import register\_converters as \_register\_converters
Using TensorFlow backend.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}


    \hypertarget{audio-feature-extraction-with-librosa}{%
\subsection{Audio Feature Extraction with
Librosa}\label{audio-feature-extraction-with-librosa}}

The key to audio classification is to extract the correct features. In
addition to \texttt{keras}, we will need the \texttt{librosa} package.
The \texttt{librosa} package in python has a rich set of methods
extracting the features of audio samples commonly used in machine
learning tasks such as speech recognition and sound classification.

Installation instructions and complete documentation for the package are
given on the \href{https://librosa.github.io/librosa/}{librosa main
page}. On most systems, you should be able to simply use:

\begin{verbatim}
pip install -u librosa
\end{verbatim}

For Unix, you may need to load some additional packages:

\begin{verbatim}
sudo apt-get install build-essential
sudo apt-get install libxext-dev python-qt4 qt4-dev-tools
pip install librosa
\end{verbatim}

After you have installed the package, try to import it.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k+kn}{import} \PY{n+nn}{librosa}
        \PY{k+kn}{import} \PY{n+nn}{librosa}\PY{n+nn}{.}\PY{n+nn}{display}
        \PY{k+kn}{import} \PY{n+nn}{librosa}\PY{n+nn}{.}\PY{n+nn}{feature}
\end{Verbatim}


    In this lab, we will use a set of music samples from the website:

http://theremin.music.uiowa.edu

This website has a great set of samples for audio processing. Look on
the web for how to use the \texttt{requests.get} and \texttt{file.write}
commands to load the file at the URL provided into your working
directory.

You can play the audio sample by copying the file to your local machine
and playing it on any media player. If you listen to it you will hear a
soprano saxaphone (with vibrato) playing four notes (C, C\#, D, Eb).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{import} \PY{n+nn}{requests}
        \PY{n}{fn} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SopSax.Vib.pp.C6Eb6.aiff}\PY{l+s+s2}{\PYZdq{}}
        \PY{n}{url} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{http://theremin.music.uiowa.edu/sound files/MIS/Woodwinds/sopranosaxophone/}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n}{fn}
        
        \PY{c+c1}{\PYZsh{} TODO:  Load the file from url and save it in a file under the name fn}
        \PY{n}{r} \PY{o}{=} \PY{n}{requests}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{url}\PY{p}{)}
        \PY{n}{wf} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{n}{fn}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{wf}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{n}{r}\PY{o}{.}\PY{n}{content}\PY{p}{)}
        \PY{n}{wf}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    Next, use \texttt{librosa} command \texttt{librosa.load} to read the
audio file with filename \texttt{fn} and get the samples \texttt{y} and
sample rate \texttt{sr}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{} TODO}
        \PY{n}{y}\PY{p}{,} \PY{n}{sr} \PY{o}{=} \PY{n}{librosa}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{fn}\PY{p}{)}
\end{Verbatim}


    Extracting features from audio files is an entire subject on its own
right. A commonly used set of features are called the Mel Frequency
Cepstral Coefficients (MFCCs). These are derived from the so-called mel
spectrogram which is something like a regular spectrogram, but the power
and frequency are represented in log scale, which more naturally aligns
with human perceptual processing. You can run the code below to display
the mel spectrogram from the audio sample.

You can easily see the four notes played in the audio track. You also
see the `harmonics' of each notes, which are other tones at integer
multiples of the fundamental frequency of each note.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{S} \PY{o}{=} \PY{n}{librosa}\PY{o}{.}\PY{n}{feature}\PY{o}{.}\PY{n}{melspectrogram}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{n}{y}\PY{p}{,} \PY{n}{sr}\PY{o}{=}\PY{n}{sr}\PY{p}{,} \PY{n}{n\PYZus{}mels}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{fmax}\PY{o}{=}\PY{l+m+mi}{8000}\PY{p}{)}
        \PY{n}{librosa}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{specshow}\PY{p}{(}\PY{n}{librosa}\PY{o}{.}\PY{n}{power\PYZus{}to\PYZus{}db}\PY{p}{(}\PY{n}{S}\PY{p}{,}\PY{n}{ref}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{)}\PY{p}{,}
                                 \PY{n}{y\PYZus{}axis}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fmax}\PY{o}{=}\PY{l+m+mi}{8000}\PY{p}{,} \PY{n}{x\PYZus{}axis}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{n+nb}{format}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}+2.0f}\PY{l+s+s1}{ dB}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mel spectrogram}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_11_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{downloading-the-data}{%
\subsection{Downloading the Data}\label{downloading-the-data}}

Using the MFCC features described above, Eric Humphrey and Juan Bellow
have created a complete data set that can used for instrument
classification. Essentially, they collected a number of data files from
the website above. For each audio file, the segmented the track into
notes and then extracted 120 MFCCs for each note. The goal is to
recognize the instrument from the 120 MFCCs. The process of feature
extraction is quite involved. So, we will just use their processed data
provided at:

https://github.com/marl/dl4mir-tutorial/blob/master/README.md

Note the password. Load the four files into some directory, say
\texttt{instrument\_dataset}. Then, load them with the commands.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{data\PYZus{}dir} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{instrument\PYZus{}dataset/}\PY{l+s+s1}{\PYZsq{}}
        \PY{n}{Xtr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{data\PYZus{}dir}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{uiowa\PYZus{}train\PYZus{}data.npy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{ytr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{data\PYZus{}dir}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{uiowa\PYZus{}train\PYZus{}labels.npy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{Xts} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{data\PYZus{}dir}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{uiowa\PYZus{}test\PYZus{}data.npy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{yts} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{data\PYZus{}dir}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{uiowa\PYZus{}test\PYZus{}labels.npy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    Looking at the data files: * What are the number of training and test
samples? * What is the number of features for each sample? * How many
classes (i.e.~instruments) are there per class.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{} TODO}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of training samples: }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{ytr}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of test samples: }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{yts}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of features: }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{Xtr}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Classes: }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{yts}\PY{p}{)}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Number of training samples: 66247
Number of test samples: 14904
Number of features: 120
Classes: 10

    \end{Verbatim}

    Before continuing, you must scale the training and test data,
\texttt{Xtr} and \texttt{Xts}. Compute the mean and std deviation of
each feature in \texttt{Xtr} and create a new training data set,
\texttt{Xtr\_scale}, by subtracting the mean and dividing by the std
deviation. Also compute a scaled test data set, \texttt{Xts\_scale}
using the mean and std deviation learned from the training data set.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{} TODO Scale the training and test matrices}
        \PY{n}{Xtr\PYZus{}scale} \PY{o}{=} \PY{p}{(}\PY{n}{Xtr} \PY{o}{\PYZhy{}} \PY{n}{Xtr}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{Xtr}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{n}{Xts\PYZus{}scale} \PY{o}{=} \PY{p}{(}\PY{n}{Xts} \PY{o}{\PYZhy{}} \PY{n}{Xts}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{Xts}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


    \hypertarget{building-a-neural-network-classifier}{%
\subsection{Building a Neural Network
Classifier}\label{building-a-neural-network-classifier}}

Following the example in \href{./mnist_neural.ipynb}{MNIST neural
network demo}, clear the keras session. Then, create a neural network
\texttt{model} with: * \texttt{nh=256} hidden units * \texttt{sigmoid}
activation for the hidden layer, \texttt{softmax} for the output layer *
select the input and output shapes correctly * print the model summary

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Model}\PY{p}{,} \PY{n}{Sequential}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Activation}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{c+c1}{\PYZsh{} TODO clear session}
         \PY{k+kn}{import} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{backend} \PY{k}{as} \PY{n+nn}{K}
         \PY{n}{K}\PY{o}{.}\PY{n}{clear\PYZus{}session}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} TODO: construct the model}
         \PY{n}{nh} \PY{o}{=} \PY{l+m+mi}{256}
         \PY{n}{nin} \PY{o}{=} \PY{n}{Xtr\PYZus{}scale}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
         \PY{n}{nout} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{yts}\PY{p}{)}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         
         \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{nh}\PY{p}{,} \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{n}{nin}\PY{p}{,}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sigmoid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{nout}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{} TODO:  Print the model summary}
         \PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
dense\_1 (Dense)              (None, 256)               30976     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_2 (Dense)              (None, 10)                2570      
=================================================================
Total params: 33,546
Trainable params: 33,546
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    To keep track of the loss history and validation accuracy, we can use a
\emph{callback} function as described in
\href{https://keras.io/callbacks/}{Keras callback documentation}. A
callback is a class that is passed to the \texttt{fit} method. Here we
provide the definition of \texttt{LoadHistory} callback class below to
save the loss and validation accuracy. This callback class allows you to
record the loss at the batch level in addition to at the epoch level.
However, for this lab, you could choose to just use the returned history
class by model.fit, which will allow you to plot the metrics at the
epoch level. For your own practice, you could use this callback class
instead of the returned history class to plot the results required
below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{k}{class} \PY{n+nc}{LossHistory}\PY{p}{(}\PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{Callback}\PY{p}{)}\PY{p}{:}
             \PY{k}{def} \PY{n+nf}{on\PYZus{}train\PYZus{}begin}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{logs}\PY{o}{=}\PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} TODO:  Create four empty lists, self.loss, self.acc, self.val\PYZus{}acc and self.batch\PYZus{}loss}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{loss} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{acc} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{val\PYZus{}acc} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{batch\PYZus{}loss} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
             \PY{k}{def} \PY{n+nf}{on\PYZus{}batch\PYZus{}end}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{batch}\PY{p}{,} \PY{n}{logs}\PY{o}{=}\PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} TODO:  This is called at the end of each batch.  }
                 \PY{c+c1}{\PYZsh{} Add the loss in logs.get(\PYZsq{}loss\PYZsq{}) to the batch\PYZus{}loss list}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{batch\PYZus{}loss}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{logs}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
                 
             \PY{k}{def} \PY{n+nf}{on\PYZus{}epoch\PYZus{}end}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{epoch}\PY{p}{,} \PY{n}{logs}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} TODO:  This is called at the end of each epoch.}
                 \PY{c+c1}{\PYZsh{} Add the traing accuracy in logs.get(\PYZsq{}acc\PYZsq{}) to the acc list}
                 \PY{c+c1}{\PYZsh{} Add the test accuracy in logs.get(\PYZsq{}val\PYZus{}acc\PYZsq{}) to the val\PYZus{}acc list}
                 \PY{c+c1}{\PYZsh{} Add the training loss in logs.get(\PYZsq{}loss\PYZsq{}) to the loss list}
                 
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{acc}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{logs}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} 
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{val\PYZus{}acc}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{logs}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} 
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{loss}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{logs}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Create an instance of the history callback}
         \PY{n}{history\PYZus{}cb} \PY{o}{=} \PY{n}{LossHistory}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    Create an optimizer and compile the model. Select the appropriate loss
function and metrics. For the optimizer, use the Adam optimizer with a
learning rate of 0.001

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{} TODO}
         \PY{k+kn}{from} \PY{n+nn}{keras} \PY{k}{import} \PY{n}{optimizers}
         \PY{n}{opt} \PY{o}{=} \PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{opt}\PY{p}{,} \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sparse\PYZus{}categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    Fit the model for 10 epochs using the scaled data for both the training
and validation. Use the \texttt{validation\_data} option to pass the
test data. Also, use the return from model.fit to record the history of
loss, accuracy, and validation accuracy in successive epochs. Use a
batch size of 100. Your final accuracy should be \textgreater{}99\%. If
you want, you could also use the callback class you defined to record
the history.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{c+c1}{\PYZsh{} TODO}
         \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{100}
         \PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{10}
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtr\PYZus{}scale}\PY{p}{,} \PY{n}{ytr}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{n}{epochs}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{history\PYZus{}cb}\PY{p}{]}\PY{p}{,} \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{Xts\PYZus{}scale}\PY{p}{,} \PY{n}{yts}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 66247 samples, validate on 14904 samples
Epoch 1/10
66247/66247 [==============================] - 1s 23us/step - loss: 0.3667 - acc: 0.8987 - val\_loss: 0.1803 - val\_acc: 0.9589
Epoch 2/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.1034 - acc: 0.9750 - val\_loss: 0.1193 - val\_acc: 0.9636
Epoch 3/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0604 - acc: 0.9854 - val\_loss: 0.1062 - val\_acc: 0.9648
Epoch 4/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0425 - acc: 0.9893 - val\_loss: 0.1035 - val\_acc: 0.9643
Epoch 5/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0324 - acc: 0.9914 - val\_loss: 0.1305 - val\_acc: 0.9516
Epoch 6/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0253 - acc: 0.9935 - val\_loss: 0.1391 - val\_acc: 0.9481
Epoch 7/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0213 - acc: 0.9945 - val\_loss: 0.1266 - val\_acc: 0.9556
Epoch 8/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0174 - acc: 0.9953 - val\_loss: 0.1247 - val\_acc: 0.9547
Epoch 9/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0149 - acc: 0.9962 - val\_loss: 0.1425 - val\_acc: 0.9495
Epoch 10/10
66247/66247 [==============================] - 1s 19us/step - loss: 0.0131 - acc: 0.9965 - val\_loss: 0.1470 - val\_acc: 0.9498

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}16}]:} <keras.callbacks.History at 0x10810bc88>
\end{Verbatim}
            
    Plot the training loss, accuracy and validation accuracy vs.~epoch,
using the returned history record from model.fit. You should produce two
subplots. One subplot contains the curve of loss vs.~epochs. The other
subplot contains two curves, one for the training accuracy and another
for the validation accuracy. You should see that the loss continuously
decreases, the accuracy continuously increases, but the validation
accuracy saturates at a little higher than 99\%. After that it ``bounces
around'' due to the noise in the stochastic gradient descent.

You coluld also try to plot the loss values saved in the
\texttt{history\_cb} class. In addition to plot the metrics for every
epoch, you could plot the \texttt{batch\_loss} per batch. But your may
want to plot the x-axis in epochs. Note that the epoch in step
\texttt{i} is \texttt{epoch\ =\ i*batch\_size/ntr} where
\texttt{batch\_size} is the batch\_size and \texttt{ntr} is the total
number of training samples.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{c+c1}{\PYZsh{} TODO}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{history\PYZus{}cb}\PY{o}{.}\PY{n}{val\PYZus{}acc}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{step} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{history\PYZus{}cb}\PY{o}{.}\PY{n}{loss}\PY{p}{)}\PY{p}{)}
         \PY{n}{step} \PY{o}{=} \PY{n}{step} \PY{o}{*} \PY{n}{batch\PYZus{}size} \PY{o}{/} \PY{n}{ytr}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{semilogy}\PY{p}{(}\PY{n}{step}\PY{p}{,} \PY{n}{history\PYZus{}cb}\PY{o}{.}\PY{n}{loss}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_30_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_30_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{optimizing-the-learning-rate}{%
\subsection{Optimizing the Learning
Rate}\label{optimizing-the-learning-rate}}

One challenge in training neural networks is the selection of the
learning rate. Rerun the above code, trying three learning rates as
shown in the vector \texttt{rates}. For each learning rate: * clear the
session * construct the network * select the optimizer. Use the Adam
optimizer with the appropriate learrning rate. * train the model * save
the accuracy and losses

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{rates} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.01}\PY{p}{,}\PY{l+m+mf}{0.001}\PY{p}{,}\PY{l+m+mf}{0.0001}\PY{p}{]}
         \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{100}
         \PY{n}{loss\PYZus{}hist} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{val\PYZus{}acc\PYZus{}hist} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} TODO}
         \PY{n}{nh} \PY{o}{=} \PY{l+m+mi}{256}
         \PY{n}{nin} \PY{o}{=} \PY{n}{Xtr\PYZus{}scale}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
         \PY{n}{nout} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{yts}\PY{p}{)}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} for each learning rate}
         \PY{k}{for} \PY{n}{rate} \PY{o+ow}{in} \PY{n}{rates}\PY{p}{:}
         
             \PY{c+c1}{\PYZsh{} clearing the session}
             \PY{n}{K}\PY{o}{.}\PY{n}{clear\PYZus{}session}\PY{p}{(}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} constructing the network}
             \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{nh}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{n}{nin}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Activation}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sigmoid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{nout}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Activation}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} selecting the optimizer}
             \PY{n}{opt} \PY{o}{=} \PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{lr}\PY{o}{=}\PY{n}{rate}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{opt}\PY{p}{,} \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sparse\PYZus{}categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{}train the model}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtr\PYZus{}scale}\PY{p}{,} \PY{n}{ytr}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{history\PYZus{}cb}\PY{p}{]}\PY{p}{,} \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{Xts\PYZus{}scale}\PY{p}{,}\PY{n}{yts}\PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} saving accuracy and losses}
             \PY{n}{loss\PYZus{}hist}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{history\PYZus{}cb}\PY{o}{.}\PY{n}{loss}\PY{p}{)}
             \PY{n}{val\PYZus{}acc\PYZus{}hist}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{history\PYZus{}cb}\PY{o}{.}\PY{n}{val\PYZus{}acc}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 66247 samples, validate on 14904 samples
Epoch 1/10
66247/66247 [==============================] - 1s 22us/step - loss: 0.1018 - acc: 0.9686 - val\_loss: 0.1804 - val\_acc: 0.9410
Epoch 2/10
66247/66247 [==============================] - 1s 19us/step - loss: 0.0278 - acc: 0.9912 - val\_loss: 0.3399 - val\_acc: 0.9045
Epoch 3/10
66247/66247 [==============================] - 1s 19us/step - loss: 0.0205 - acc: 0.9933 - val\_loss: 0.2989 - val\_acc: 0.9195
Epoch 4/10
66247/66247 [==============================] - 1s 19us/step - loss: 0.0198 - acc: 0.9934 - val\_loss: 0.2758 - val\_acc: 0.9317
Epoch 5/10
66247/66247 [==============================] - 1s 19us/step - loss: 0.0161 - acc: 0.9949 - val\_loss: 0.3150 - val\_acc: 0.9266
Epoch 6/10
66247/66247 [==============================] - 1s 19us/step - loss: 0.0194 - acc: 0.9944 - val\_loss: 0.2916 - val\_acc: 0.9323
Epoch 7/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0129 - acc: 0.9955 - val\_loss: 0.3912 - val\_acc: 0.9266
Epoch 8/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0111 - acc: 0.9966 - val\_loss: 0.4867 - val\_acc: 0.9003
Epoch 9/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0120 - acc: 0.9962 - val\_loss: 0.3922 - val\_acc: 0.9276
Epoch 10/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0144 - acc: 0.9956 - val\_loss: 0.4726 - val\_acc: 0.9166
Train on 66247 samples, validate on 14904 samples
Epoch 1/10
66247/66247 [==============================] - 2s 23us/step - loss: 0.3606 - acc: 0.9026 - val\_loss: 0.2015 - val\_acc: 0.9495
Epoch 2/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.1017 - acc: 0.9751 - val\_loss: 0.1393 - val\_acc: 0.9550
Epoch 3/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0598 - acc: 0.9855 - val\_loss: 0.1241 - val\_acc: 0.9575
Epoch 4/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0422 - acc: 0.9893 - val\_loss: 0.1324 - val\_acc: 0.9498
Epoch 5/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0318 - acc: 0.9918 - val\_loss: 0.1417 - val\_acc: 0.9448
Epoch 6/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0254 - acc: 0.9933 - val\_loss: 0.1506 - val\_acc: 0.9424
Epoch 7/10
66247/66247 [==============================] - 1s 21us/step - loss: 0.0203 - acc: 0.9944 - val\_loss: 0.1545 - val\_acc: 0.9416
Epoch 8/10
66247/66247 [==============================] - 1s 21us/step - loss: 0.0171 - acc: 0.9958 - val\_loss: 0.1376 - val\_acc: 0.9500
Epoch 9/10
66247/66247 [==============================] - 1s 21us/step - loss: 0.0148 - acc: 0.9963 - val\_loss: 0.1788 - val\_acc: 0.9363
Epoch 10/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.0130 - acc: 0.9967 - val\_loss: 0.1729 - val\_acc: 0.9397
Train on 66247 samples, validate on 14904 samples
Epoch 1/10
66247/66247 [==============================] - 2s 25us/step - loss: 1.1038 - acc: 0.6649 - val\_loss: 0.7939 - val\_acc: 0.7181
Epoch 2/10
66247/66247 [==============================] - 1s 21us/step - loss: 0.5412 - acc: 0.8548 - val\_loss: 0.5088 - val\_acc: 0.8668
Epoch 3/10
66247/66247 [==============================] - 1s 22us/step - loss: 0.3732 - acc: 0.9136 - val\_loss: 0.3798 - val\_acc: 0.9123
Epoch 4/10
66247/66247 [==============================] - 1s 22us/step - loss: 0.2885 - acc: 0.9354 - val\_loss: 0.3021 - val\_acc: 0.9368
Epoch 5/10
66247/66247 [==============================] - 1s 21us/step - loss: 0.2346 - acc: 0.9478 - val\_loss: 0.2523 - val\_acc: 0.9421
Epoch 6/10
66247/66247 [==============================] - 1s 21us/step - loss: 0.1960 - acc: 0.9554 - val\_loss: 0.2165 - val\_acc: 0.9554
Epoch 7/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.1666 - acc: 0.9612 - val\_loss: 0.1904 - val\_acc: 0.9573
Epoch 8/10
66247/66247 [==============================] - 1s 22us/step - loss: 0.1433 - acc: 0.9662 - val\_loss: 0.1736 - val\_acc: 0.9571
Epoch 9/10
66247/66247 [==============================] - 1s 20us/step - loss: 0.1246 - acc: 0.9705 - val\_loss: 0.1571 - val\_acc: 0.9591
Epoch 10/10
66247/66247 [==============================] - 1s 21us/step - loss: 0.1095 - acc: 0.9742 - val\_loss: 0.1542 - val\_acc: 0.9550

    \end{Verbatim}

    Plot the loss funciton vs.~the epoch number for all three learning rates
on one graph. You should see that the lower learning rates are more
stable, but converge slower. Similarly, plot the accuracy and validation
accuracy and make your observations. You may want to plot the three set
of figures as three subplots.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{} TODO}
         \PY{n}{step} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{history\PYZus{}cb}\PY{o}{.}\PY{n}{loss}\PY{p}{)}\PY{p}{)}
         \PY{n}{step} \PY{o}{=} \PY{n}{step} \PY{o}{*} \PY{n}{batch\PYZus{}size} \PY{o}{/} \PY{n}{ytr}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{semilogy}\PY{p}{(}\PY{n}{step}\PY{p}{,} \PY{n}{loss\PYZus{}hist}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{semilogy}\PY{p}{(}\PY{n}{step}\PY{p}{,} \PY{n}{loss\PYZus{}hist}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{semilogy}\PY{p}{(}\PY{n}{step}\PY{p}{,} \PY{n}{loss\PYZus{}hist}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{(}\PY{n}{rates}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{rates}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{rates}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}19}]:} <matplotlib.legend.Legend at 0x11f033198>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_34_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Question: Which learning rate is the best ?

    Answer: From the figure above, we can see that the loss of a lower
learning rate is more stable, but it converges slower. So generally, a
lower learning rate is better to reach a stable result. However, we
could choose larger learning rate to help us to save time in traning.
For example, in this case, 0.001 may be the best learning rate, since it
balanced the stability and the rate of convergence.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
